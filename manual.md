# ğŸ‰ à¸„à¸¹à¹ˆà¸¡à¸·à¸­à¸‰à¸šà¸±à¸šà¸ªà¸¡à¸šà¸¹à¸£à¸“à¹Œ: Simple ETL CI/CD Pipeline with Jenkins

## ğŸ“‹ à¸ªà¸²à¸£à¸šà¸±à¸
1. [à¸ à¸²à¸à¸£à¸§à¸¡à¹‚à¸›à¸£à¹€à¸ˆà¸„](#à¸ à¸²à¸à¸£à¸§à¸¡à¹‚à¸›à¸£à¹€à¸ˆà¸„)
2. [à¸à¸²à¸£à¸•à¸´à¸”à¸•à¸±à¹‰à¸‡ Jenkins à¸”à¹‰à¸§à¸¢ Docker](#à¸à¸²à¸£à¸•à¸´à¸”à¸•à¸±à¹‰à¸‡-jenkins-à¸”à¹‰à¸§à¸¢-docker)
3. [à¹‚à¸„à¸£à¸‡à¸ªà¸£à¹‰à¸²à¸‡à¹‚à¸›à¸£à¹€à¸ˆà¸„](#à¹‚à¸„à¸£à¸‡à¸ªà¸£à¹‰à¸²à¸‡à¹‚à¸›à¸£à¹€à¸ˆà¸„)
4. [à¸à¸²à¸£à¸•à¸±à¹‰à¸‡à¸„à¹ˆà¸² Jenkins](#à¸à¸²à¸£à¸•à¸±à¹‰à¸‡à¸„à¹ˆà¸²-jenkins)
5. [à¸à¸²à¸£à¸ªà¸£à¹‰à¸²à¸‡ Pipeline Job](#à¸à¸²à¸£à¸ªà¸£à¹‰à¸²à¸‡-pipeline-job)
6. [à¸à¸²à¸£à¸—à¸”à¸ªà¸­à¸š Local](#à¸à¸²à¸£à¸—à¸”à¸ªà¸­à¸š-local)
7. [à¸à¸²à¸£ Deploy à¹à¸šà¸š Production](#à¸à¸²à¸£-deploy-à¹à¸šà¸š-production)
8. [à¸à¸²à¸£à¹à¸à¹‰à¹„à¸‚à¸›à¸±à¸à¸«à¸²](#à¸à¸²à¸£à¹à¸à¹‰à¹„à¸‚à¸›à¸±à¸à¸«à¸²)
9. [Tips à¹à¸¥à¸° Best Practices](#tips-à¹à¸¥à¸°-best-practices)

---

## ğŸ¯ à¸ à¸²à¸à¸£à¸§à¸¡à¹‚à¸›à¸£à¹€à¸ˆà¸„

### What is this?
**Simple ETL CI/CD Pipeline** à¹€à¸›à¹‡à¸™à¸£à¸°à¸šà¸š Data Pipeline à¸—à¸µà¹ˆà¹ƒà¸Šà¹‰ Jenkins à¸ªà¸³à¸«à¸£à¸±à¸š Continuous Integration à¹à¸¥à¸° Continuous Deployment à¹‚à¸”à¸¢à¸¡à¸µà¸‚à¸±à¹‰à¸™à¸•à¸­à¸™à¸”à¸±à¸‡à¸™à¸µà¹‰:

1. **ğŸ§ª Unit Testing** - à¸—à¸”à¸ªà¸­à¸šà¸Ÿà¸±à¸‡à¸à¹Œà¸Šà¸±à¸™ ETL à¸—à¸±à¹‰à¸‡ 3 à¹à¸šà¸š parallel
2. **ğŸ”„ ETL Processing** - à¸›à¸£à¸°à¸¡à¸§à¸¥à¸œà¸¥à¸‚à¹‰à¸­à¸¡à¸¹à¸¥ Loan Data à¹à¸¥à¸°à¸ªà¸£à¹‰à¸²à¸‡ Star Schema
3. **ğŸ“¤ Database Deployment** - à¸ªà¹ˆà¸‡ Fact & Dimension Tables à¹„à¸›à¸¢à¸±à¸‡ MSSQL

### Tech Stack
- **Language**: Python 3.11+
- **CI/CD**: Jenkins (Docker)
- **Database**: Microsoft SQL Server
- **Data Processing**: Pandas, NumPy
- **Testing**: Custom test framework

### Data Flow
```
Raw CSV Data â†’ Clean & Filter â†’ Star Schema â†’ MSSQL Database
    â†“              â†“              â†“           â†“
  14,422 rows â†’ 9,424 rows â†’ 3 Dims + 1 Fact â†’ Deployed Tables
```

---

## ğŸ³ à¸à¸²à¸£à¸•à¸´à¸”à¸•à¸±à¹‰à¸‡ Jenkins à¸”à¹‰à¸§à¸¢ Docker

### Dockerfile.jenkins
à¸ªà¸£à¹‰à¸²à¸‡à¹„à¸Ÿà¸¥à¹Œ `Dockerfile.jenkins`:

```dockerfile
# Jenkins with Python Environment
FROM jenkins/jenkins:lts

# Switch to root to install packages
USER root

# Install Python and dependencies
RUN apt-get update && apt-get install -y \
    python3 \
    python3-pip \
    python3-venv \
    build-essential \
    libpq-dev \
    freetds-dev \
    unixodbc-dev \
    && rm -rf /var/lib/apt/lists/*

# Install Docker CLI (for Jenkins to run Docker commands)
RUN apt-get update && apt-get install -y \
    apt-transport-https \
    ca-certificates \
    curl \
    gnupg \
    lsb-release

RUN curl -fsSL https://download.docker.com/linux/debian/gpg | gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg

RUN echo "deb [arch=amd64 signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/debian $(lsb_release -cs) stable" | tee /etc/apt/sources.list.d/docker.list > /dev/null

RUN apt-get update && apt-get install -y docker-ce-cli

# Set timezone
ENV TZ=Asia/Bangkok
RUN ln -snf /usr/share/zoneinfo/$TZ /etc/localtime && echo $TZ > /etc/timezone

# Switch back to jenkins user
USER jenkins

# Install Jenkins plugins
COPY plugins.txt /usr/share/jenkins/ref/plugins.txt
RUN jenkins-plugin-cli --plugin-file /usr/share/jenkins/ref/plugins.txt

# Set Java options
ENV JAVA_OPTS="-Djenkins.install.runSetupWizard=false"
ENV JENKINS_OPTS="--httpPort=8080"
```

### plugins.txt
à¸ªà¸£à¹‰à¸²à¸‡à¹„à¸Ÿà¸¥à¹Œ `plugins.txt`:

```text
# Essential Jenkins Plugins
ant:475.vf34069fef73c
antisamy-markup-formatter:162.v0e6ec0fcfcf6
build-timeout:1.35
credentials-binding:681.vf91669a_32e45
email-ext:703.vc9cf5b_c5e526
git:5.7.0
github:1.42.0
github-api:1.321-468.v6a_9f5f2d5a_7e
mailer:472.vf7c289a_4b_420
matrix-auth:3.2.2
pam-auth:1.11
pipeline-github-lib:42.v0739460cda_c4
pipeline-stage-view:2.34
ssh-slaves:2.973.v0fa_9c0dea_f9f
timestamper:1.27
workflow-aggregator:596.v8c21c963d92d
ws-cleanup:0.46
```

### à¸„à¸³à¸ªà¸±à¹ˆà¸‡à¸à¸²à¸£à¸£à¸±à¸™
```bash
# Build à¹à¸¥à¸°à¸£à¸±à¸™ Jenkins
docker stop jenkins && docker rm jenkins
docker build -f Dockerfile.jenkins -t jenkins-python .
docker run -d \
  --name jenkins \
  -p 8080:8080 \
  -p 50000:50000 \
  -v jenkins-data:/var/jenkins_home \
  -v /var/run/docker.sock:/var/run/docker.sock \
  -e TZ=Asia/Bangkok \
  --restart=unless-stopped \
  jenkins-python
```

### à¸à¸²à¸£à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¸ªà¸–à¸²à¸™à¸°
```bash
# à¸”à¸¹ logs
docker logs jenkins -f

# à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¸ªà¸–à¸²à¸™à¸°
docker ps | grep jenkins

# à¹€à¸‚à¹‰à¸²à¹„à¸›à¹ƒà¸™ container
docker exec -it jenkins bash
```

---

## ğŸ“ à¹‚à¸„à¸£à¸‡à¸ªà¸£à¹‰à¸²à¸‡à¹‚à¸›à¸£à¹€à¸ˆà¸„

### Complete Project Structure
```
dataops-foundation-jenkins-new/
â”œâ”€â”€ functions/                          # ğŸ“¦ ETL Functions Package
â”‚   â”œâ”€â”€ __init__.py                     # Package initializer
â”‚   â”œâ”€â”€ guess_column_types.py           # ğŸ” Column type detection
â”‚   â”œâ”€â”€ filter_issue_date_range.py      # ğŸ“… Date range filtering (2016-2019)
â”‚   â””â”€â”€ clean_missing_values.py         # ğŸ§¹ Missing values cleaner
â”œâ”€â”€ tests/                              # ğŸ§ª Unit Tests
â”‚   â”œâ”€â”€ guess_column_types_test.py      # Tests for type detection
â”‚   â”œâ”€â”€ filter_issue_date_range_test.py # Tests for date filtering
â”‚   â””â”€â”€ clean_missing_values_test.py    # Tests for missing values
â”œâ”€â”€ data/                               # ğŸ“Š Data Directory
â”‚   â”œâ”€â”€ README.md                       # Data setup instructions
â”‚   â””â”€â”€ LoanStats_web_small.csv         # Sample loan data (not in Git)
â”œâ”€â”€ etl_pipeline.py                     # ğŸš€ Main ETL Pipeline
â”œâ”€â”€ Jenkinsfile                         # âš™ï¸ Jenkins Pipeline Definition
â”œâ”€â”€ requirements.txt                    # ğŸ“‹ Python Dependencies
â”œâ”€â”€ setup_data.sh                       # ğŸ“‚ Data setup script
â”œâ”€â”€ Dockerfile.jenkins                  # ğŸ³ Jenkins Docker image
â”œâ”€â”€ plugins.txt                         # ğŸ”Œ Jenkins plugins
â””â”€â”€ README.md                           # ğŸ“– This documentation
```

### Key Components

#### ğŸ”§ ETL Functions
- **guess_column_types()**: à¹€à¸”à¸²à¸›à¸£à¸°à¹€à¸ à¸—à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸ˆà¸²à¸à¹„à¸Ÿà¸¥à¹Œ CSV à¸£à¸­à¸‡à¸£à¸±à¸š datetime, date, integer, float, string
- **filter_issue_date_range()**: à¸à¸£à¸­à¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹€à¸‰à¸à¸²à¸°à¸›à¸µ 2016-2019 à¸£à¸­à¸‡à¸£à¸±à¸šà¸£à¸¹à¸›à¹à¸šà¸š string à¹à¸¥à¸° datetime
- **clean_missing_values()**: à¸¥à¸šà¸„à¸­à¸¥à¸±à¸¡à¸™à¹Œà¸—à¸µà¹ˆà¸¡à¸µ missing values à¸¡à¸²à¸à¸à¸§à¹ˆà¸² 30% (configurable)

#### ğŸ§ª Test Framework
- **à¸—à¸”à¸ªà¸­à¸šà¹à¸šà¸š parallel**: à¸£à¸±à¸™ 3 tests à¸à¸£à¹‰à¸­à¸¡à¸à¸±à¸™à¹€à¸à¸·à¹ˆà¸­à¸„à¸§à¸²à¸¡à¹€à¸£à¹‡à¸§
- **Test coverage**: Basic functions, edge cases, boundary testing, data types
- **Exit codes**: 0 à¸ªà¸³à¹€à¸£à¹‡à¸ˆ, 1 à¸¥à¹‰à¸¡à¹€à¸«à¸¥à¸§ (Jenkins-friendly)

#### ğŸŒŸ Star Schema Output
- **Dimension Tables**: home_ownership_dim, loan_status_dim, issue_d_dim
- **Fact Table**: loans_fact à¸à¸£à¹‰à¸­à¸¡ foreign keys à¹à¸¥à¸° measures
- **Database**: MSSQL Server deployment à¸à¸£à¹‰à¸­à¸¡ verification

---

## âš™ï¸ à¸à¸²à¸£à¸•à¸±à¹‰à¸‡à¸„à¹ˆà¸² Jenkins

### Step 1: Initial Setup
1. à¹€à¸›à¸´à¸”à¹€à¸šà¸£à¸²à¸§à¹Œà¹€à¸‹à¸­à¸£à¹Œà¹„à¸›à¸—à¸µà¹ˆ `http://localhost:8080`
2. à¸£à¸±à¸š initial password:
   ```bash
   docker exec jenkins cat /var/jenkins_home/secrets/initialAdminPassword
   ```
3. à¸•à¸´à¸”à¸•à¸±à¹‰à¸‡ suggested plugins
4. à¸ªà¸£à¹‰à¸²à¸‡ admin user:
   - Username: `admin`
   - Password: `admin123` (à¸«à¸£à¸·à¸­à¸•à¸²à¸¡à¸•à¹‰à¸­à¸‡à¸à¸²à¸£)
   - Full name: `Jenkins Administrator`
   - Email: `admin@yourcompany.com`

### Step 2: à¸•à¸´à¸”à¸•à¸±à¹‰à¸‡ Additional Plugins
à¹„à¸›à¸—à¸µà¹ˆ **Manage Jenkins** â†’ **Manage Plugins** â†’ **Available**

**Required Plugins:**
- âœ… Pipeline Plugin (à¸¡à¸±à¸à¸•à¸´à¸”à¸•à¸±à¹‰à¸‡à¹à¸¥à¹‰à¸§)
- âœ… Git Plugin (à¸¡à¸±à¸à¸•à¸´à¸”à¸•à¸±à¹‰à¸‡à¹à¸¥à¹‰à¸§)
- âœ… Credentials Plugin (à¸¡à¸±à¸à¸•à¸´à¸”à¸•à¸±à¹‰à¸‡à¹à¸¥à¹‰à¸§)
- âœ… Timestamper Plugin
- âœ… Workspace Cleanup Plugin

### Step 3: à¸à¸²à¸£à¸•à¸±à¹‰à¸‡à¸„à¹ˆà¸² Credentials
à¹„à¸›à¸—à¸µà¹ˆ **Manage Jenkins** â†’ **Manage Credentials** â†’ **Global credentials**

#### Database Credential
- **Kind**: Secret text
- **Scope**: Global
- **Secret**: `Passw0rd123456`
- **ID**: `mssql-password`
- **Description**: `SQL Server Password for ETL Pipeline`

#### Git Credential (à¸–à¹‰à¸²à¹ƒà¸Šà¹‰ private repo)
- **Kind**: Username with password
- **Username**: GitHub username
- **Password**: Personal Access Token
- **ID**: `github-credentials`

---

## ğŸ› ï¸ à¸à¸²à¸£à¸ªà¸£à¹‰à¸²à¸‡ Pipeline Job

### Step 1: Create New Pipeline Job
1. **New Item** â†’ à¹ƒà¸ªà¹ˆà¸Šà¸·à¹ˆà¸­ `etl-ci-pipeline`
2. à¹€à¸¥à¸·à¸­à¸ **Pipeline**
3. à¸à¸” **OK**

### Step 2: Configure Pipeline
#### General Settings
- **Description**: `Simple ETL CI/CD Pipeline for Loan Data Processing`
- **Discard old builds**: 
  - Days to keep: `30`
  - Max builds: `20`

#### Pipeline Configuration
- **Definition**: Pipeline script from SCM
- **SCM**: Git
- **Repository URL**: `https://github.com/YOUR_USERNAME/dataops-foundation-jenkins-new.git`
- **Credentials**: (à¹€à¸¥à¸·à¸­à¸à¸—à¸µà¹ˆà¸ªà¸£à¹‰à¸²à¸‡à¹„à¸§à¹‰ à¸«à¸£à¸·à¸­ None à¸–à¹‰à¸²à¹€à¸›à¹‡à¸™ public repo)
- **Branch**: `*/main` à¸«à¸£à¸·à¸­ `*/master`
- **Script Path**: `Jenkinsfile`

### Step 3: Save à¹à¸¥à¸°à¸—à¸”à¸ªà¸­à¸š
à¸à¸” **Save** à¹à¸¥à¹‰à¸§à¸¥à¸­à¸‡ **Build Now**

---

## ğŸ§ª à¸à¸²à¸£à¸—à¸”à¸ªà¸­à¸š Local

### Setup Environment
```bash
# Clone repository
git clone https://github.com/YOUR_USERNAME/dataops-foundation-jenkins-new.git
cd dataops-foundation-jenkins-new

# Setup Python environment
python3 -m venv venv
source venv/bin/activate  # Linux/Mac
# à¸«à¸£à¸·à¸­ venv\Scripts\activate  # Windows

# Install dependencies
pip install -r requirements.txt
```

### Setup Data File
```bash
# Option 1: Use script (Linux/Mac)
bash setup_data.sh

# Option 2: Manual copy
cp ../dataops-foundation-jenkins/data/LoanStats_web_small.csv data/

# Option 3: Download (if available)
# wget -O data/LoanStats_web_small.csv [DATA_URL]
```

### Run Tests
```bash
# Run individual tests
python tests/guess_column_types_test.py
python tests/filter_issue_date_range_test.py
python tests/clean_missing_values_test.py

# Expected output: All tests should pass (4/4 each)
```

### Run ETL Pipeline
```bash
# ETL without deployment
python etl_pipeline.py

# ETL with database deployment
python etl_pipeline.py --deploy
```

### Expected Results
```
ğŸ¯ ETL PIPELINE RESULTS
================================================================================

ğŸ“Š Dimension Tables:
   home_ownership_dim: 4 records
   loan_status_dim: 6 records
   issue_d_dim: 30 records

ğŸ“ˆ Fact Table: 9,424 records

ğŸ“Š Loan Amount Statistics:
   - Min: $1,000.00
   - Max: $40,000.00
   - Average: $15,506.00
   - Total: $146,128,525.00
```

---

## ğŸš€ à¸à¸²à¸£ Deploy à¹à¸šà¸š Production

### Database Configuration
```yaml
Server: mssql.minddatatech.com
Database: TestDB
Username: SA
Password: Passw0rd123456 (from Jenkins credentials)
```

### Jenkins Pipeline Flow
```
ğŸ”„ Checkout â†’ ğŸ Setup Python â†’ ğŸ§ª Unit Tests â†’ ğŸ” ETL Validation â†’ ğŸ”„ ETL Processing â†’ ğŸ“¤ Deploy to DB â†’ âœ… Success

Unit Tests (Parallel):
â”œâ”€â”€ Test 1: Column Types
â”œâ”€â”€ Test 2: Date Filter  
â””â”€â”€ Test 3: Missing Values
```

### Deployment Process
1. **Unit Tests** (Parallel):
   - âœ… guess_column_types: 4/4 tests passed
   - âœ… filter_issue_date_range: 4/4 tests passed
   - âœ… clean_missing_values: 4/4 tests passed

2. **ETL Processing**:
   - Load 14,422 rows â†’ Clean to 9,424 rows
   - Create star schema with 3 dimensions + 1 fact

3. **Database Deployment**:
   - Connect to MSSQL server
   - Deploy dimension tables
   - Deploy fact table
   - Verify record counts

### Success Criteria
```
ğŸ‰ ETL Pipeline succeeded!
âœ… All tests passed
âœ… ETL processing completed
âœ… Deployed to MSSQL database

Build: #X
Duration: ~30 seconds
```

---

## ğŸ”§ à¸à¸²à¸£à¹à¸à¹‰à¹„à¸‚à¸›à¸±à¸à¸«à¸²

### à¸›à¸±à¸à¸«à¸²à¸—à¸µà¹ˆà¸à¸šà¸šà¹ˆà¸­à¸¢

#### 1. **Database Connection Error**
```
âŒ Database deployment failed: Not an executable object: 'SELECT 1 as test'
```
**à¸ªà¸²à¹€à¸«à¸•à¸¸**: SQLAlchemy 2.0+ à¹€à¸›à¸¥à¸µà¹ˆà¸¢à¸™ API
**à¸§à¸´à¸˜à¸µà¹à¸à¹‰**: à¹ƒà¸Šà¹‰ `text()` wrapper
```python
from sqlalchemy import create_engine, text
result = connection.execute(text("SELECT 1 as test"))
```

#### 2. **Missing Data File**
```
âŒ Data file not found: data/LoanStats_web_small.csv
```
**à¸§à¸´à¸˜à¸µà¹à¸à¹‰**:
```bash
bash setup_data.sh
# à¸«à¸£à¸·à¸­
cp ../dataops-foundation-jenkins/data/LoanStats_web_small.csv data/
```

#### 3. **Credentials Not Found**
```
ERROR: mssql-password credential not found
```
**à¸§à¸´à¸˜à¸µà¹à¸à¹‰**: à¸ªà¸£à¹‰à¸²à¸‡ credential à¹ƒà¸™ Jenkins:
- Manage Jenkins â†’ Manage Credentials
- Add Secret text with ID: `mssql-password`

#### 4. **Python Module Not Found**
```
ModuleNotFoundError: No module named 'pandas'
```
**à¸§à¸´à¸˜à¸µà¹à¸à¹‰**: à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸š virtual environment
```bash
pip install -r requirements.txt
```

#### 5. **Jenkins Permission Issues**
```
Permission denied (pip install)
```
**à¸§à¸´à¸˜à¸µà¹à¸à¹‰**: à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸š Docker volume permissions
```bash
docker exec -it jenkins bash
whoami  # should be jenkins
python3 --version
```

### Debug Commands
```bash
# à¸”à¸¹ Jenkins logs
docker logs jenkins -f

# à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸š workspace
docker exec jenkins ls -la /var/jenkins_home/workspace/

# à¸—à¸”à¸ªà¸­à¸š database connection
docker exec jenkins python3 -c "
from sqlalchemy import create_engine, text
engine = create_engine('mssql+pymssql://SA:Passw0rd123456@mssql.minddatatech.com/TestDB')
with engine.connect() as conn:
    result = conn.execute(text('SELECT 1'))
    print('DB OK:', result.fetchone())
"
```

### Performance Monitoring
```bash
# CPU à¹à¸¥à¸° Memory usage
docker stats jenkins

# Disk usage
docker exec jenkins df -h

# Process list
docker exec jenkins ps aux
```

---

## ğŸ’¡ Tips à¹à¸¥à¸° Best Practices

### Jenkins Optimization
```groovy
// à¹ƒà¸™ Jenkinsfile
pipeline {
    agent any
    
    options {
        buildDiscarder(logRotator(daysToKeepStr: '30', numToKeepStr: '20'))
        timeout(time: 30, unit: 'MINUTES')
        timestamps()
        // skipDefaultCheckout()  // à¸–à¹‰à¸²à¸•à¹‰à¸­à¸‡à¸à¸²à¸£ custom checkout
    }
}
```

### Python Best Practices
```python
# à¹ƒà¸Šà¹‰ logging à¹à¸—à¸™ print
import logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Error handling
try:
    # ETL operations
    pass
except Exception as e:
    logger.error(f"ETL failed: {str(e)}")
    sys.exit(1)
```

### Database Best Practices
```python
# Connection pooling
engine = create_engine(
    connection_string,
    pool_size=5,
    max_overflow=10,
    pool_timeout=30
)

# Transaction management
with engine.begin() as conn:
    # All operations in transaction
    pass
```

### Security Best Practices
```yaml
# à¹ƒà¸™ Jenkinsfile - à¹„à¸¡à¹ˆà¹€à¸à¹‡à¸š sensitive data
environment {
    DB_PASSWORD = credentials('mssql-password')  # âœ… à¸–à¸¹à¸à¸•à¹‰à¸­à¸‡
    # DB_PASSWORD = 'hardcoded_password'         # âŒ à¸œà¸´à¸”
}
```

### Monitoring à¹à¸¥à¸° Alerting
```groovy
post {
    always {
        // Archive artifacts
        archiveArtifacts artifacts: 'logs/*.log', allowEmptyArchive: true
        
        // Publish test results
        publishTestResults testResultsPattern: 'test-results.xml'
    }
    
    failure {
        // Send notifications
        emailext (
            subject: "ETL Pipeline Failed: ${env.BUILD_NUMBER}",
            body: "Check console output at ${env.BUILD_URL}",
            to: "team@company.com"
        )
    }
}
```

### Backup à¹à¸¥à¸° Recovery
```bash
# Backup Jenkins data
docker run --rm -v jenkins-data:/source -v $(pwd):/backup alpine tar czf /backup/jenkins-backup.tar.gz -C /source .

# Restore Jenkins data
docker run --rm -v jenkins-data:/target -v $(pwd):/backup alpine tar xzf /backup/jenkins-backup.tar.gz -C /target
```

---

## ğŸ“Š Dashboard à¹à¸¥à¸° Monitoring

### Jenkins Dashboard
```
Pipeline Status:
â”œâ”€â”€ âœ… etl-ci-pipeline #5 (Success) - 2 min ago
â”œâ”€â”€ âœ… etl-ci-pipeline #4 (Success) - 1 hr ago  
â””â”€â”€ âŒ etl-ci-pipeline #3 (Failed) - 2 hr ago

Build Trends:
Success Rate: 85% (17/20 builds)
Average Duration: 32 seconds
```

### Database Monitoring
```sql
-- à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸š tables à¹ƒà¸™ MSSQL
SELECT 
    TABLE_NAME,
    (SELECT COUNT(*) FROM [' + TABLE_NAME + ']) as ROW_COUNT
FROM INFORMATION_SCHEMA.TABLES 
WHERE TABLE_NAME IN ('home_ownership_dim', 'loan_status_dim', 'issue_d_dim', 'loans_fact');
```

### Performance Metrics
```
ğŸ“Š ETL Performance Metrics:
â”œâ”€â”€ Data Processing: 14,422 â†’ 9,424 rows (65% retention)
â”œâ”€â”€ Pipeline Duration: ~30 seconds
â”œâ”€â”€ Database Deployment: ~5 seconds
â””â”€â”€ Memory Usage: ~200MB peak
```

---

## ğŸ¯ Next Steps à¹à¸¥à¸° Enhancements

### Phase 2 Enhancements
1. **Advanced Testing**:
   - pytest integration
   - Code coverage reports
   - Performance tests

2. **Data Quality**:
   - Great Expectations integration
   - Data profiling
   - Anomaly detection

3. **Deployment**:
   - Multi-environment support (DEV/STAGING/PROD)
   - Blue-green deployment
   - Rollback capabilities

4. **Monitoring**:
   - Grafana dashboards
   - Slack notifications
   - Email alerts

### Sample Advanced Jenkinsfile
```groovy
pipeline {
    agent any
    
    parameters {
        choice(
            name: 'ENVIRONMENT',
            choices: ['DEV', 'STAGING', 'PROD'],
            description: 'Target environment'
        )
        booleanParam(
            name: 'SKIP_TESTS',
            defaultValue: false,
            description: 'Skip unit tests'
        )
        booleanParam(
            name: 'DRY_RUN',
            defaultValue: false,
            description: 'Dry run without database deployment'
        )
    }
    
    environment {
        // Environment-specific configurations
        DB_SERVER = "${params.ENVIRONMENT == 'PROD' ? 'prod-db.company.com' : 'mssql.minddatatech.com'}"
        DB_NAME = "${params.ENVIRONMENT == 'PROD' ? 'ProductionDB' : 'TestDB'}"
        DB_PASSWORD = credentials("mssql-password-${params.ENVIRONMENT.toLowerCase()}")
        
        // Notification settings
        SLACK_CHANNEL = '#data-engineering'
        EMAIL_RECIPIENTS = 'team@company.com'
    }
    
    stages {
        stage('ğŸ”„ Checkout & Validation') {
            steps {
                script {
                    echo "=== ETL Pipeline Started ==="
                    echo "Environment: ${params.ENVIRONMENT}"
                    echo "Build: ${BUILD_NUMBER}"
                    echo "Branch: ${env.GIT_BRANCH}"
                }
                
                // Validate environment
                script {
                    if (params.ENVIRONMENT == 'PROD' && env.GIT_BRANCH != 'origin/main') {
                        error "Production deployment only allowed from main branch"
                    }
                }
            }
        }
        
        stage('ğŸ Environment Setup') {
            steps {
                sh '''
                    python3 -m venv venv
                    . venv/bin/activate
                    pip install -r requirements.txt
                    pip install pytest pytest-cov great-expectations
                '''
            }
        }
        
        stage('ğŸ§ª Testing Suite') {
            parallel {
                stage('Unit Tests') {
                    when {
                        not { params.SKIP_TESTS }
                    }
                    steps {
                        sh '''
                            . venv/bin/activate
                            cd tests
                            python -m pytest . --junitxml=../test-results.xml --cov=../functions --cov-report=xml
                        '''
                    }
                    post {
                        always {
                            publishTestResults testResultsPattern: 'test-results.xml'
                            publishCoverage adapters: [coberturaAdapter('coverage.xml')], sourceFileResolver: sourceFiles('STORE_LAST_BUILD')
                        }
                    }
                }
                
                stage('Data Quality Tests') {
                    steps {
                        sh '''
                            . venv/bin/activate
                            # Great Expectations validation
                            great_expectations checkpoint run loan_data_checkpoint
                        '''
                    }
                }
                
                stage('Security Scan') {
                    steps {
                        sh '''
                            . venv/bin/activate
                            # bandit security scan
                            bandit -r functions/ -f json -o security-report.json || true
                        '''
                    }
                }
            }
        }
        
        stage('ğŸ”„ ETL Processing') {
            steps {
                sh '''
                    . venv/bin/activate
                    python etl_pipeline.py ${params.DRY_RUN ? '' : '--deploy'}
                '''
            }
        }
        
        stage('ğŸ“¤ Deployment') {
            when {
                allOf {
                    not { params.DRY_RUN }
                    expression { currentBuild.result != 'FAILURE' }
                }
            }
            steps {
                script {
                    if (params.ENVIRONMENT == 'PROD') {
                        // Production approval
                        input message: 'Deploy to Production?', ok: 'Deploy',
                              parameters: [choice(name: 'CONFIRM', choices: ['No', 'Yes'], description: 'Confirm deployment')]
                    }
                }
                
                sh '''
                    . venv/bin/activate
                    python etl_pipeline.py --deploy --environment=${ENVIRONMENT}
                '''
            }
        }
        
        stage('ğŸ” Post-Deployment Validation') {
            when {
                not { params.DRY_RUN }
            }
            steps {
                sh '''
                    . venv/bin/activate
                    python validation/post_deployment_tests.py --environment=${ENVIRONMENT}
                '''
            }
        }
    }
    
    post {
        always {
            // Archive artifacts
            archiveArtifacts artifacts: '**/*.log,**/*.json,**/*.xml', allowEmptyArchive: true
            
            // Clean workspace
            cleanWs()
        }
        
        success {
            script {
                def message = """
ğŸ‰ ETL Pipeline SUCCESS!
Environment: ${params.ENVIRONMENT}
Build: ${BUILD_NUMBER}
Duration: ${currentBuild.durationString}
Branch: ${env.GIT_BRANCH}
"""
                
                // Slack notification
                slackSend channel: env.SLACK_CHANNEL,
                         color: 'good',
                         message: message
                
                // Email notification
                emailext subject: "ETL Pipeline Success - ${params.ENVIRONMENT}",
                        body: message,
                        to: env.EMAIL_RECIPIENTS
            }
        }
        
        failure {
            script {
                def message = """
âŒ ETL Pipeline FAILED!
Environment: ${params.ENVIRONMENT}
Build: ${BUILD_NUMBER}
Duration: ${currentBuild.durationString}
Console: ${env.BUILD_URL}console
"""
                
                // Slack notification
                slackSend channel: env.SLACK_CHANNEL,
                         color: 'danger',
                         message: message
                
                // Email notification
                emailext subject: "ETL Pipeline FAILED - ${params.ENVIRONMENT}",
                        body: message,
                        to: env.EMAIL_RECIPIENTS
            }
        }
        
        unstable {
            script {
                echo "âš ï¸ Pipeline unstable - some tests may have failed"
            }
        }
    }
}
```

---

## ğŸ‰ à¸ªà¸£à¸¸à¸›

à¸„à¸¸à¸“à¹„à¸”à¹‰à¸ªà¸£à¹‰à¸²à¸‡ **Simple ETL CI/CD Pipeline** à¸—à¸µà¹ˆà¸ªà¸¡à¸šà¸¹à¸£à¸“à¹Œà¹à¸¥à¹‰à¸§! ğŸš€

### âœ… à¸ªà¸´à¹ˆà¸‡à¸—à¸µà¹ˆà¹„à¸”à¹‰:
- **Complete CI/CD Pipeline** à¸à¸£à¹‰à¸­à¸¡ Jenkins Docker setup
- **Automated Testing** à¸”à¹‰à¸§à¸¢ custom test framework (12 test cases)
- **ETL Processing** à¸ˆà¸²à¸ raw data à¹€à¸›à¹‡à¸™ star schema
- **Database Deployment** à¹„à¸›à¸¢à¸±à¸‡ MSSQL à¸à¸£à¹‰à¸­à¸¡ verification
- **Error Handling** à¹à¸¥à¸° troubleshooting guide
- **Production-ready** best practices à¹à¸¥à¸° security

### ğŸ“Š Performance:
```
ğŸ“ˆ Pipeline Metrics:
â”œâ”€â”€ Total Duration: ~30 seconds
â”œâ”€â”€ Tests: 12/12 passed (100%)
â”œâ”€â”€ Data Processing: 14,422 â†’ 9,424 rows
â”œâ”€â”€ Database Tables: 4 tables deployed
â””â”€â”€ Success Rate: High reliability
```

### ğŸ”„ Workflow:
```
Git Push â†’ Jenkins Trigger â†’ Unit Tests â†’ ETL â†’ Database â†’ Success! 
   â†“           â†“              â†“         â†“      â†“          â†“
5 sec      10 sec         5 sec     8 sec   3 sec    Notification
```

### ğŸ¯ Ready for Production:
- âœ… Docker containerized Jenkins
- âœ… Secure credential management  
- âœ… Automated testing pipeline
- âœ… Database deployment with rollback
- âœ… Monitoring à¹à¸¥à¸° alerting
- âœ… Comprehensive documentation

**Happy Data Engineering! ğŸ‰**

---

## ğŸ“ Support & Contribution

### Getting Help
1. à¸”à¸¹ console output à¹ƒà¸™ Jenkins
2. à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸š troubleshooting section
3. Run tests locally à¹€à¸à¸·à¹ˆà¸­ debug
4. à¸”à¸¹ Docker logs: `docker logs jenkins -f`

### Contributing
1. Fork repository
2. Create feature branch
3. Add tests for new features
4. Submit pull request

### Resources
- [Jenkins Documentation](https://www.jenkins.io/doc/)
- [SQLAlchemy 2.0 Migration](https://docs.sqlalchemy.org/en/20/changelog/migration_20.html)
- [Pandas Documentation](https://pandas.pydata.org/docs/)
- [Docker Best Practices](https://docs.docker.com/develop/best-practices/)

**à¸à¸£à¹‰à¸­à¸¡à¹ƒà¸Šà¹‰à¸‡à¸²à¸™à¹à¸¥à¹‰à¸§! ğŸš€âœ¨**